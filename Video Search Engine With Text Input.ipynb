{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da16b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Project Steps\n",
    "#1) Identify your video sources: Organize them in such you with id and their url so you can identify them later on. \n",
    "    #Also keep in mind your hardware resources. Better to use smaller videos e.g. 1-5 mins.\n",
    "\n",
    "#2) Transcribe your videos: Convert your videos to text. \n",
    "    #If you don't have transcription tool you can extract audio from a video using opensource tools like \"shotcut\". \n",
    "    #And then you can use Open AI whisper to transcribe.\n",
    "\n",
    "#3) Create a semantic text search back end: You will find semantic search technique \n",
    "    #taught in this course useful for this purpose.\n",
    "\n",
    "#4) Push to GitHub and share your repository with the class in a comment/question to this assignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f93604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0) Install moviepy\n",
    "#!pip install moviepy==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import moviepy.editor as mp\n",
    "print(moviepy.editor.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38b5957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) extract audio from a video using moviepy==1.0.3\n",
    "\n",
    "\n",
    "import os\n",
    "import moviepy.editor as mp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_audio_from_video_in_folder(input_folder_path):\n",
    "    \n",
    "    input_folder_path = r\"C:\\Users\\imeyalo\\Downloads\\Videos\"  # Replace with your video file path\n",
    "    output_folder_path = r\"C:\\Users\\imeyalo\\Downloads\\Audio\" # Replace with your desired output audio file path\n",
    "    \n",
    "    if not os.path.isdir(input_folder_path):\n",
    "        print(f\"Error: Folder '{input_folder_path}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        for filename in os.listdir(input_folder_path):\n",
    "            filepath = os.path.join(input_folder_path, filename)\n",
    "            outputfilepath = os.path.join(output_folder_path, filename)\n",
    "            \n",
    "            # Load the video clip\n",
    "            video_clip = mp.VideoFileClip(filepath)\n",
    "\n",
    "            # Extract the audio from the video clip\n",
    "            audio_clip = video_clip.audio\n",
    "                       \n",
    "       \n",
    "            \n",
    "            # Check if audio is available and write to output folder\n",
    "            if audio_clip is not None:\n",
    "                \n",
    "                audio_clip.write_audiofile(outputfilepath,codec=\"libmp3lame\")\n",
    "                \n",
    "            \n",
    "            else:\n",
    "                print(\"No audio track found in the video.\")\n",
    "\n",
    "                    \n",
    "\n",
    "            # Close the clips\n",
    "            if audio_clip is not None:\n",
    "                audio_clip.close()\n",
    "                \n",
    "            if video_clip is not None:    \n",
    "                video_clip.close()\n",
    "\n",
    "            print(f\"Audio extracted successfully to {output_folder_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "# Example usage:\n",
    "extract_audio_from_video_in_folder(r\"C:\\Users\\imeyalo\\Downloads\\Videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e19fd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing: 1_854877-hd_1920_1080_25fps.mp4\n",
      "Saved to: C:\\Users\\imeyalo\\Downloads\\Transcripts_json\\1_854877-hd_1920_1080_25fps.json\n",
      "Transcribing: 2_1585618-hd_1280_720_30fps.mp4\n",
      "Saved to: C:\\Users\\imeyalo\\Downloads\\Transcripts_json\\2_1585618-hd_1280_720_30fps.json\n",
      "Transcribing: 3_4438080-hd_1920_1080_25fps.mp4\n",
      "Saved to: C:\\Users\\imeyalo\\Downloads\\Transcripts_json\\3_4438080-hd_1920_1080_25fps.json\n",
      "Transcribing: 4_4.mp4\n",
      "Saved to: C:\\Users\\imeyalo\\Downloads\\Transcripts_json\\4_4.json\n",
      "Transcribing: 5_10182004-hd_3240_2160_24fps.mp4\n",
      "Saved to: C:\\Users\\imeyalo\\Downloads\\Transcripts_json\\5_10182004-hd_3240_2160_24fps.json\n"
     ]
    }
   ],
   "source": [
    "#2)Transcribe your videos: Convert your videos to text. \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Input/output folders\n",
    "input_folder = r\"C:\\Users\\imeyalo\\Downloads\\Audio\"\n",
    "output_folder = r\"C:\\Users\\imeyalo\\Downloads\\Transcripts_json\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Audio file formats supported by Whisper API\n",
    "valid_extensions = (\".mp3\", \".mp4\", \".m4a\", \".wav\", \".webm\")\n",
    "\n",
    "# Process all audio files\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.lower().endswith(valid_extensions):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        output_path = os.path.join(output_folder, f\"{base_name}.json\")\n",
    "\n",
    "        print(f\"Transcribing: {filename}\")\n",
    "\n",
    "        try:\n",
    "            with open(input_path, \"rb\") as audio_file:\n",
    "                # Use verbose_json to include timestamps\n",
    "                transcript = openai.Audio.transcribe(\n",
    "                    model=\"whisper-1\",\n",
    "                    file=audio_file,\n",
    "                    response_format=\"verbose_json\"\n",
    "                )\n",
    "\n",
    "            # Save full timestamped transcript as JSON\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(transcript, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"Saved to: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error transcribing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "565cec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 5 text chunks.\n",
      "Chunks saved to chunked_transcripts.json\n"
     ]
    }
   ],
   "source": [
    "#3)Extract and chunk segments from each json file\n",
    "#use the segments array to group them into larger text blocks. This is useful for: \n",
    "#Reducing the number of embeddings\n",
    "#Improving semantic coherence in search\n",
    "#Better handling of long transcripts\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Settings\n",
    "input_folder = r\"C:\\Users\\imeyalo\\Downloads\\Transcripts_json\"\n",
    "chunk_size = 3  # Number of segments per chunk (adjust as needed)\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        filepath = os.path.join(input_folder, filename)\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            segments = data.get(\"segments\", [])\n",
    "\n",
    "            # Chunk the segments\n",
    "            for i in range(0, len(segments), chunk_size):\n",
    "                chunk_segments = segments[i:i + chunk_size]\n",
    "\n",
    "                # Combine text and calculate time range\n",
    "                chunk_text = \" \".join(seg[\"text\"].strip() for seg in chunk_segments)\n",
    "                start_time = chunk_segments[0][\"start\"]\n",
    "                end_time = chunk_segments[-1][\"end\"]\n",
    "\n",
    "                # Store the chunk with metadata\n",
    "                all_chunks.append({\n",
    "                    \"file\": filename,\n",
    "                    \"start\": start_time,\n",
    "                    \"end\": end_time,\n",
    "                    \"text\": chunk_text\n",
    "                })\n",
    "\n",
    "print(f\"Extracted {len(all_chunks)} text chunks.\")\n",
    "\n",
    "# (Optional) Save chunks to a file\n",
    "with open(r\"C:\\Users\\imeyalo\\Downloads\\chunked_transcripts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Chunks saved to chunked_transcripts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a2de10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_854877-hd_1920_1080_25fps.json: 190 tokens\n",
      "2_1585618-hd_1280_720_30fps.json: 210 tokens\n",
      "3_4438080-hd_1920_1080_25fps.json: 190 tokens\n",
      "4_4.json: 224 tokens\n",
      "5_10182004-hd_3240_2160_24fps.json: 190 tokens\n",
      "\n",
      " Average tokens per file: 200.80\n"
     ]
    }
   ],
   "source": [
    "#Average token per file\n",
    "\n",
    "import os\n",
    "import tiktoken\n",
    "\n",
    "# Set the path to your folder with .txt transcript files\n",
    "folder_path = r\"C:\\Users\\imeyalo\\Downloads\\Transcripts_json\"  # Change if needed\n",
    "\n",
    "# Choose tokenizer model (e.g., gpt-3.5-turbo, gpt-4, etc.)\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "token_counts = []\n",
    "\n",
    "# Loop through all .txt files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            tokens = tokenizer.encode(text)\n",
    "            count = len(tokens)\n",
    "            token_counts.append(count)\n",
    "            print(f\"{filename}: {count} tokens\")\n",
    "\n",
    "# Compute average\n",
    "if token_counts:\n",
    "    avg_tokens = sum(token_counts) / len(token_counts)\n",
    "    print(f\"\\n Average tokens per file: {avg_tokens:.2f}\")\n",
    "else:\n",
    "    print(\"No .json files found in the folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7903e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 5 vectors.\n",
      "All chunks uploaded to Pinecone.\n"
     ]
    }
   ],
   "source": [
    "#4)Upload Embeddings to Pinecone\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import openai\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tqdm.autonotebook import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "#pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_env = os.getenv(\"PINECONE_ENVIRONMENT\")\n",
    "pinecone_index_name = os.getenv(\"PINECONE_INDEX\")\n",
    "\n",
    "# Initialize Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "if pinecone_index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name='audio-search',\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(\n",
    "                cloud='aws',\n",
    "                region='us-east-1'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "index = pc.Index(pinecone_index_name)\n",
    "\n",
    "# Load chunked transcript file\n",
    "with open( r\"C:\\Users\\imeyalo\\Downloads\\chunked_transcripts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "# Get embeddings from OpenAI\n",
    "def get_embedding(text):\n",
    "    response = openai.Embedding.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "# Upload chunks to Pinecone in batches\n",
    "batch_size = 25\n",
    "for i in range(0, len(chunks), batch_size):\n",
    "    batch = chunks[i:i + batch_size]\n",
    "    vectors = []\n",
    "    for item in batch:\n",
    "        vector_id = str(uuid.uuid4())\n",
    "        embedding = get_embedding(item[\"text\"])\n",
    "        metadata = {\n",
    "            \"file\": item[\"file\"],\n",
    "            \"start\": item[\"start\"],\n",
    "            \"end\": item[\"end\"],\n",
    "            \"text\": item[\"text\"]\n",
    "        }\n",
    "        vectors.append((vector_id, embedding, metadata))\n",
    "    index.upsert(vectors)\n",
    "    print(f\"Uploaded {len(vectors)} vectors.\")\n",
    "\n",
    "print(\"All chunks uploaded to Pinecone.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c1d87a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test search:\n",
      "[0.0s - 9.5s] (4_4.json)\n",
      "Thank you for watching the video.\n",
      "---\n",
      "[0.0s - 23.5s] (2_1585618-hd_1280_720_30fps.json)\n",
      "Thank you for watching!\n",
      "---\n",
      "[0.0s - 2.059999942779541s] (3_4438080-hd_1920_1080_25fps.json)\n",
      "you\n",
      "---\n",
      "[0.0s - 2.059999942779541s] (5_10182004-hd_3240_2160_24fps.json)\n",
      "you\n",
      "---\n",
      "[0.0s - 2.059999942779541s] (1_854877-hd_1920_1080_25fps.json)\n",
      "you\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Query Pinecone with Semantic Search\n",
    "\n",
    "def semantic_search(query, top_k=5):\n",
    "    query_embedding = get_embedding(query)\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "    return results[\"matches\"]\n",
    "\n",
    "# Test search\n",
    "print(\"\\n Test search:\")\n",
    "query = \"Thank you\"\n",
    "results = semantic_search(query)\n",
    "\n",
    "for match in results:\n",
    "    meta = match[\"metadata\"]\n",
    "    print(f\"[{meta['start']}s - {meta['end']}s] ({meta['file']})\")\n",
    "    print(meta['text'])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf36f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "References\n",
    "1. https://www.pexels.com\n",
    "2. https://www.pixabay.com\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VSE",
   "language": "python",
   "name": "vse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
